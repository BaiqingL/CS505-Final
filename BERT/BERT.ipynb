{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2c9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb516223",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c261c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6afdb",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bd901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/2017_English_final/GOLD/Subtask_A/'\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for file_name in files:\n",
    "        if 'train' in file_name and '.txt' in file_name:\n",
    "            train_files.append(os.path.join(data_dir, file_name))\n",
    "        if 'dev' in file_name and '.txt' in file_name:\n",
    "            val_files.append(os.path.join(data_dir, file_name))\n",
    "        if 'test' in file_name and '.txt' in file_name:\n",
    "            test_files.append(os.path.join(data_dir, file_name))\n",
    "        \n",
    "train_data = []\n",
    "train_labels = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "sentiment_to_label = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "\n",
    "for file_path in train_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            entries = l.split('\\t')\n",
    "            train_data.append(entries[2])\n",
    "            train_labels.append(sentiment_to_label[entries[1]])\n",
    "            \n",
    "    \n",
    "for file_path in val_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            entries = l.split('\\t')\n",
    "            val_data.append(entries[2])\n",
    "            val_labels.append(sentiment_to_label[entries[1]])\n",
    "            \n",
    "\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f965c",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328a5196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d568a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  149\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for tweet in np.concatenate((train_data, val_data)):\n",
    "    \n",
    "    if len(tweet) > 280:\n",
    "        print('The following entry is longer than the max tweet length:')\n",
    "        print(tweet)\n",
    "    \n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(tweet, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bca3184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr4/cs542sp/baiqing/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1,  ..., 1, 2, 1])\n",
      "tensor([1, 1, 0,  ..., 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "\n",
    "\n",
    "def tokenize_data(data, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    # For every sentence...\n",
    "    for tweet in data:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            tweet,                      # Tweet to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_len+100,  # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    print(labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels \n",
    "\n",
    "\n",
    "input_ids_train, attention_masks_train, labels_train = tokenize_data(train_data, train_labels)\n",
    "input_ids_val, attention_masks_val, labels_val = tokenize_data(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e37ed6",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643fa290",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb315ee",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42ca584d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 3, # The number of output labels--3 for 3 classes.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a908b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the torch implementation of AdamW as the huggingface version is deprecated\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac896cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5af053",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd8a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70c129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694ec66",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ce4348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    506.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    506.    Elapsed: 0:00:33.\n",
      "  Batch   120  of    506.    Elapsed: 0:00:50.\n",
      "  Batch   160  of    506.    Elapsed: 0:01:07.\n",
      "  Batch   200  of    506.    Elapsed: 0:01:24.\n",
      "  Batch   240  of    506.    Elapsed: 0:01:41.\n",
      "  Batch   280  of    506.    Elapsed: 0:01:58.\n",
      "  Batch   320  of    506.    Elapsed: 0:02:15.\n",
      "  Batch   360  of    506.    Elapsed: 0:02:31.\n",
      "  Batch   400  of    506.    Elapsed: 0:02:48.\n",
      "  Batch   440  of    506.    Elapsed: 0:03:05.\n",
      "  Batch   480  of    506.    Elapsed: 0:03:22.\n",
      "\n",
      "  Average training loss: 0.73\n",
      "  Training epcoh took: 0:03:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.58\n",
      "  Validation Loss: 0.86\n",
      "  Validation took: 0:00:25\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    506.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    506.    Elapsed: 0:00:34.\n",
      "  Batch   120  of    506.    Elapsed: 0:00:51.\n",
      "  Batch   160  of    506.    Elapsed: 0:01:08.\n",
      "  Batch   200  of    506.    Elapsed: 0:01:24.\n",
      "  Batch   240  of    506.    Elapsed: 0:01:41.\n",
      "  Batch   280  of    506.    Elapsed: 0:01:58.\n",
      "  Batch   320  of    506.    Elapsed: 0:02:15.\n",
      "  Batch   360  of    506.    Elapsed: 0:02:32.\n",
      "  Batch   400  of    506.    Elapsed: 0:02:49.\n",
      "  Batch   440  of    506.    Elapsed: 0:03:06.\n",
      "  Batch   480  of    506.    Elapsed: 0:03:23.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epcoh took: 0:03:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.63\n",
      "  Validation Loss: 0.83\n",
      "  Validation took: 0:00:25\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    506.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    506.    Elapsed: 0:00:34.\n",
      "  Batch   120  of    506.    Elapsed: 0:00:51.\n",
      "  Batch   160  of    506.    Elapsed: 0:01:08.\n",
      "  Batch   200  of    506.    Elapsed: 0:01:24.\n",
      "  Batch   240  of    506.    Elapsed: 0:01:41.\n",
      "  Batch   280  of    506.    Elapsed: 0:01:58.\n",
      "  Batch   320  of    506.    Elapsed: 0:02:15.\n",
      "  Batch   360  of    506.    Elapsed: 0:02:32.\n",
      "  Batch   400  of    506.    Elapsed: 0:02:49.\n",
      "  Batch   440  of    506.    Elapsed: 0:03:06.\n",
      "  Batch   480  of    506.    Elapsed: 0:03:23.\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:03:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.63\n",
      "  Validation Loss: 0.97\n",
      "  Validation took: 0:00:25\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    506.    Elapsed: 0:00:17.\n",
      "  Batch    80  of    506.    Elapsed: 0:00:34.\n",
      "  Batch   120  of    506.    Elapsed: 0:00:51.\n",
      "  Batch   160  of    506.    Elapsed: 0:01:08.\n",
      "  Batch   200  of    506.    Elapsed: 0:01:24.\n",
      "  Batch   240  of    506.    Elapsed: 0:01:41.\n",
      "  Batch   280  of    506.    Elapsed: 0:01:58.\n",
      "  Batch   320  of    506.    Elapsed: 0:02:15.\n",
      "  Batch   360  of    506.    Elapsed: 0:02:32.\n",
      "  Batch   400  of    506.    Elapsed: 0:02:49.\n",
      "  Batch   440  of    506.    Elapsed: 0:03:06.\n",
      "  Batch   480  of    506.    Elapsed: 0:03:23.\n",
      "\n",
      "  Average training loss: 0.25\n",
      "  Training epcoh took: 0:03:33\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.63\n",
      "  Validation Loss: 1.07\n",
      "  Validation took: 0:00:25\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:15:53 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "        # output values prior to applying an activation function like the \n",
    "        # softmax.\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f6b1a",
   "metadata": {},
   "source": [
    "### Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85613d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.730343</td>\n",
       "      <td>0.864279</td>\n",
       "      <td>0.581786</td>\n",
       "      <td>0:03:33</td>\n",
       "      <td>0:00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.515262</td>\n",
       "      <td>0.834306</td>\n",
       "      <td>0.633180</td>\n",
       "      <td>0:03:33</td>\n",
       "      <td>0:00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.357239</td>\n",
       "      <td>0.967308</td>\n",
       "      <td>0.626648</td>\n",
       "      <td>0:03:33</td>\n",
       "      <td>0:00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254411</td>\n",
       "      <td>1.068041</td>\n",
       "      <td>0.630978</td>\n",
       "      <td>0:03:33</td>\n",
       "      <td>0:00:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1           0.730343     0.864279       0.581786       0:03:33         0:00:25\n",
       "2           0.515262     0.834306       0.633180       0:03:33         0:00:25\n",
       "3           0.357239     0.967308       0.626648       0:03:33         0:00:25\n",
       "4           0.254411     1.068041       0.630978       0:03:33         0:00:25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d4fdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75511a27",
   "metadata": {},
   "source": [
    "### Prepare Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ed87cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '../data/2017_Arabic_train_final/GOLD/SemEval2017-task4-train.subtask-A.english.txt'\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "with open(test_file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        entries = l.split('\\t')\n",
    "        if len(entries) != 3:\n",
    "            entries = l.split(' ', maxsplit=2)\n",
    "        test_data.append(entries[2])\n",
    "        test_labels.append(sentiment_to_label[entries[1]])\n",
    "            \n",
    "    \n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7950bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2,  ..., 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr4/cs542sp/baiqing/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_ids_test, attention_masks_test, labels_test = tokenize_data(test_data, test_labels)\n",
    "\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00fd6882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,355 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids_test)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "    logits = result.logits\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d374b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5943368107302534\n"
     ]
    }
   ],
   "source": [
    "predictions = [p for sublist in predictions for p in sublist]\n",
    "true_labels = np.array([l for sublist in true_labels for l in sublist])\n",
    "print(flat_accuracy(predictions, true_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
